{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--7kRdNc3g1C"
   },
   "source": [
    "### Introducing Keras\n",
    "\n",
    "We will introduce [Keras](http://keras.io/), a high-level library for machine learning which we will use for the rest of the class. Keras is built on top of [Tensorflow](https://tensorflow.org/), which is an open-source framework which impelments machine learning methodology, particularly that of deep neural networks, by optimizing the efficiency of the computation. We do not have to deal so much with the details of this. For our purposes, Tensorflow is also a very low-level library which is not necessarily accessible to the typical engineer. Keras solves this by creating a wrapper around Tensorflow, reducing the complexity of coding neural networks, and giving us a set of convenient functions which implement lots of reusable routines. Most importantly, Keras (via Tensorflow) efficiently implement backpropagation to train neural networks on the GPU. Effectively, you could say that Keras is to Tensorflow what Processing is to Java.\n",
    "\n",
    "Start by importing the relevant Keras libraries that we will be using, as well as matplotlib and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAWw9rel3g1E"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3e1aed44f3da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XOFerCCt3g1G"
   },
   "source": [
    "Let's load the Iris dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0B7QPmQa3g1H"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "data, labels = iris.data[:,0:3], iris.data[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Js1nqPqI3g1K"
   },
   "source": [
    "In the last lesson, we manually trained a neural network to predict the sepal width of the Iris flowers. Let's use the Keras library. First we need to shuffle and pre-process the data. Pre-processing is normalization of the data, as well as converting it to a properly-shaped numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "y8sQ1iTc3g1K",
    "outputId": "05c2d772-40c6-4678-b1a0-65aa7e3e2fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X (150, 3)\n",
      "first 5 rows of X\n",
      " [[0.7848101  0.5        0.65217394]\n",
      " [0.8101266  0.72727275 0.76811594]\n",
      " [0.73417723 0.59090906 0.5797101 ]\n",
      " [0.62025315 0.5681818  0.65217394]\n",
      " [0.8101266  0.72727275 0.65217394]]\n",
      "first 5 labels\n",
      " [0.6  0.92 0.48 0.68 0.6 ]\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(labels)  # size of our dataset\n",
    "shuffle_order = np.random.permutation(num_samples)\n",
    "data = data[shuffle_order, :]\n",
    "labels = labels[shuffle_order]\n",
    "\n",
    "# normalize data and labels to between 0 and 1 and make sure it's float32\n",
    "data = data / np.amax(data, axis=0)\n",
    "data = data.astype('float32')\n",
    "labels = labels / np.amax(labels, axis=0)\n",
    "labels = labels.astype('float32')\n",
    "\n",
    "# print out the data\n",
    "print(\"shape of X\", data.shape)\n",
    "print(\"first 5 rows of X\\n\", data[0:5, :])\n",
    "print(\"first 5 labels\\n\", labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwbYBSI83g1N"
   },
   "source": [
    "### Overfitting and validation\n",
    "\n",
    "If we evaluate the performance of the network on the same data that we trained it on, it is wrong; our network could learn to \"cheat\" by overfitting to the training data (like memorizing it) so as to get a high score, but then not generalize well to actually unknown examples.\n",
    "\n",
    "In machine learning, this is called \"overfitting\" and there are several things we have to do to avoid it. The first thing is we must split our dataset into a \"training set\" which we train on with gradient descent, and a \"test set\" which is hidden from the training process that we can do a final evaluation on to get the true accuracy, that of the network trying to predict unknown samples.\n",
    "\n",
    "Let's split the data into a training set and a test set. We'll keep the first 30% of the dataset to use as a test set, and use the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wFQ8p7T23g1O",
    "outputId": "941fd8a0-7ca3-4b4d-dcf9-11fbf2bf9170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 training samples, 45 test samples\n"
     ]
    }
   ],
   "source": [
    "# let's rename the data and labels to X, y\n",
    "X, y = data, labels\n",
    "\n",
    "test_split = 0.3  # percent split\n",
    "\n",
    "n_test = int(test_split * num_samples)\n",
    "\n",
    "x_train, x_test = X[n_test:, :], X[:n_test, :] \n",
    "y_train, y_test = y[n_test:], y[:n_test] \n",
    "\n",
    "print('%d training samples, %d test samples' % (x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJHjB9-L3g1Q"
   },
   "source": [
    "In Keras, to instantiate a neural network model, we use the `Sequential` class. Sequential simply means a model with a sequence of layers which propagate in one direction, from input to output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7FLN6R63g1R"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByJ05NOU3g1T"
   },
   "source": [
    "We now have an empty neural network called `model`. Now let's add our first layer, which will be our input layer. We will do this using Keras's `Dense` class which will instantiate our input layer.\n",
    "\n",
    "The reason why it is called \"Dense\" is that the layer is \"fully-connected,\" which means that all of it's neurons are connected to all the neurons in the previous layer, with no empty connections. This may seem confusing at first because we have not yet seen neural network layers which are not fully-connected; we will see this in the next chapter when we introduce convolutional networks. \n",
    "\n",
    "To create a Dense layer, we have two arguments that need to be specified: the number of neurons and the activation function (which non-linearity, if any). For the first layer, we must also specify the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMh41MEs3g1T"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(8, activation='sigmoid', input_dim=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XP0gru033g1V"
   },
   "source": [
    "We can also get a readout of the current state of the network using `model.summary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "aH9Vng5A3g1W",
    "outputId": "6e697371-2e72-4db1-fbd3-f79ca2139261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 8)                 32        \n",
      "=================================================================\n",
      "Total params: 32\n",
      "Trainable params: 32\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Shn4hakX3g1Y"
   },
   "source": [
    "Our network currently has one layer with 32 parameters: that's 3 neurons in the input layer, times 8 neurons in the middle layer (3x8=24), plus 8 biases (24+8=32).\n",
    "\n",
    "Next, we will add the output layer, which will be a fully-connected (Dense) layer whose size is 1 neuron. This neuron will contain our final output.\n",
    "\n",
    "Notice that instead of having the activation be a sigmoid as before, we leave it as a \"linear\" activation (no non-linearity). This is common for the final output.\n",
    "\n",
    "We add it, and look at the final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "EiEysa8U3g1Y",
    "outputId": "b9aa8f76-a58d-4ed5-a1b0-52695f11f89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qktjzJg63g1a"
   },
   "source": [
    "So we've added 9 parameters, 8x1 weights between the hidden and output layers, and 1 bias in the output. So we have 41 parameters in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EWAepcf3g1b"
   },
   "source": [
    "Now we are finished specifying the architecture of the model. Now we need to specify our loss function and optimizer, and then compile the model. Let's discuss each of these things.\n",
    "\n",
    "First, we specify the loss. The standard for regression, as we said before is sum-squared error (SSE) or mean-squared error (MSE). SSE and MSE are basically the same, since the only difference between them is a scaling factor ($\\frac{1}{n}$) which doesn't depend on the final weights. Keras happens to use MSE for evaluation rather than SEE, so we will use that.\n",
    "\n",
    "The optimizer is the flavor of gradient descent we want. The most basic optimizer is \"stochastic gradient descent\" or SGD which is the learning algorithm we have used so far. We have mostly used batch gradient descent so far, which means we compute our gradient over the entire dataset. For reasons which will be more clear when we cover learning algorithms in more detail, this is not usually favored, and we instead calculate the gradient over random subsets of the training data, called mini-batches.\n",
    "\n",
    "Once we've specified our loss function and optimizer, the model is compiled. Compiling means that Keras (actually Tensorflow internally) is allocating memory for a \"computational graph\" whose architecture is that which is specified by your model definition. This is done for optimization purposes, and a full understanding of how that's done is not necessary for this course and is beyond its scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mah397Ri3g1c"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBmKq0963g1e"
   },
   "source": [
    "We are finally ready to train. In the next cell, we run the `fit` command which will begin the process of training. There are several important arguments to `fit`. The first is the training data and labels (`x_train` and `y_train`), as well as the validation set (`x_test` and `y_test`). \n",
    "\n",
    "Additionally, we must specify the `batch_size` which refers to the number of training samples to calculate the gradient over (using SGD), as well as the number of `epochs`, which refers to the number of times we cycle through the training set. In general, more epochs are usually better, although in practice, the accuracy of the network may stop improving early, which makes it unnecessary to train for too many epochs.\n",
    "\n",
    "Because we have a very small dataset (just 105 samples), we should have a low batch size and can afford to train over many epochs (let's set to 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_R5lgBIX3g1e",
    "outputId": "e14b9644-4457-482c-8111-d56517ab99ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/200\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.0642 - val_loss: 0.0450\n",
      "Epoch 2/200\n",
      "105/105 [==============================] - 0s 817us/step - loss: 0.0620 - val_loss: 0.0453\n",
      "Epoch 3/200\n",
      "105/105 [==============================] - 0s 797us/step - loss: 0.0612 - val_loss: 0.0435\n",
      "Epoch 4/200\n",
      "105/105 [==============================] - 0s 863us/step - loss: 0.0597 - val_loss: 0.0462\n",
      "Epoch 5/200\n",
      "105/105 [==============================] - 0s 845us/step - loss: 0.0591 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "105/105 [==============================] - 0s 919us/step - loss: 0.0582 - val_loss: 0.0437\n",
      "Epoch 7/200\n",
      "105/105 [==============================] - 0s 974us/step - loss: 0.0569 - val_loss: 0.0415\n",
      "Epoch 8/200\n",
      "105/105 [==============================] - 0s 959us/step - loss: 0.0563 - val_loss: 0.0420\n",
      "Epoch 9/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0549 - val_loss: 0.0396\n",
      "Epoch 10/200\n",
      "105/105 [==============================] - 0s 898us/step - loss: 0.0537 - val_loss: 0.0389\n",
      "Epoch 11/200\n",
      "105/105 [==============================] - 0s 893us/step - loss: 0.0527 - val_loss: 0.0375\n",
      "Epoch 12/200\n",
      "105/105 [==============================] - 0s 935us/step - loss: 0.0520 - val_loss: 0.0371\n",
      "Epoch 13/200\n",
      "105/105 [==============================] - 0s 924us/step - loss: 0.0503 - val_loss: 0.0368\n",
      "Epoch 14/200\n",
      "105/105 [==============================] - 0s 890us/step - loss: 0.0499 - val_loss: 0.0349\n",
      "Epoch 15/200\n",
      "105/105 [==============================] - 0s 902us/step - loss: 0.0490 - val_loss: 0.0371\n",
      "Epoch 16/200\n",
      "105/105 [==============================] - 0s 803us/step - loss: 0.0481 - val_loss: 0.0359\n",
      "Epoch 17/200\n",
      "105/105 [==============================] - 0s 828us/step - loss: 0.0473 - val_loss: 0.0333\n",
      "Epoch 18/200\n",
      "105/105 [==============================] - 0s 886us/step - loss: 0.0461 - val_loss: 0.0320\n",
      "Epoch 19/200\n",
      "105/105 [==============================] - 0s 983us/step - loss: 0.0452 - val_loss: 0.0320\n",
      "Epoch 20/200\n",
      "105/105 [==============================] - 0s 895us/step - loss: 0.0440 - val_loss: 0.0311\n",
      "Epoch 21/200\n",
      "105/105 [==============================] - 0s 936us/step - loss: 0.0434 - val_loss: 0.0301\n",
      "Epoch 22/200\n",
      "105/105 [==============================] - 0s 935us/step - loss: 0.0427 - val_loss: 0.0296\n",
      "Epoch 23/200\n",
      "105/105 [==============================] - 0s 902us/step - loss: 0.0414 - val_loss: 0.0326\n",
      "Epoch 24/200\n",
      "105/105 [==============================] - 0s 840us/step - loss: 0.0412 - val_loss: 0.0289\n",
      "Epoch 25/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0398 - val_loss: 0.0297\n",
      "Epoch 26/200\n",
      "105/105 [==============================] - 0s 920us/step - loss: 0.0391 - val_loss: 0.0274\n",
      "Epoch 27/200\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.0384 - val_loss: 0.0275\n",
      "Epoch 28/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0376 - val_loss: 0.0273\n",
      "Epoch 29/200\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.0369 - val_loss: 0.0271\n",
      "Epoch 30/200\n",
      "105/105 [==============================] - 0s 873us/step - loss: 0.0364 - val_loss: 0.0254\n",
      "Epoch 31/200\n",
      "105/105 [==============================] - 0s 820us/step - loss: 0.0358 - val_loss: 0.0256\n",
      "Epoch 32/200\n",
      "105/105 [==============================] - 0s 947us/step - loss: 0.0347 - val_loss: 0.0255\n",
      "Epoch 33/200\n",
      "105/105 [==============================] - 0s 877us/step - loss: 0.0337 - val_loss: 0.0278\n",
      "Epoch 34/200\n",
      "105/105 [==============================] - 0s 889us/step - loss: 0.0334 - val_loss: 0.0234\n",
      "Epoch 35/200\n",
      "105/105 [==============================] - 0s 854us/step - loss: 0.0327 - val_loss: 0.0230\n",
      "Epoch 36/200\n",
      "105/105 [==============================] - 0s 892us/step - loss: 0.0320 - val_loss: 0.0225\n",
      "Epoch 37/200\n",
      "105/105 [==============================] - 0s 814us/step - loss: 0.0314 - val_loss: 0.0232\n",
      "Epoch 38/200\n",
      "105/105 [==============================] - 0s 814us/step - loss: 0.0307 - val_loss: 0.0231\n",
      "Epoch 39/200\n",
      "105/105 [==============================] - 0s 902us/step - loss: 0.0303 - val_loss: 0.0222\n",
      "Epoch 40/200\n",
      "105/105 [==============================] - 0s 986us/step - loss: 0.0292 - val_loss: 0.0204\n",
      "Epoch 41/200\n",
      "105/105 [==============================] - 0s 851us/step - loss: 0.0291 - val_loss: 0.0204\n",
      "Epoch 42/200\n",
      "105/105 [==============================] - 0s 876us/step - loss: 0.0283 - val_loss: 0.0198\n",
      "Epoch 43/200\n",
      "105/105 [==============================] - 0s 873us/step - loss: 0.0277 - val_loss: 0.0196\n",
      "Epoch 44/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0273 - val_loss: 0.0193\n",
      "Epoch 45/200\n",
      "105/105 [==============================] - 0s 926us/step - loss: 0.0269 - val_loss: 0.0197\n",
      "Epoch 46/200\n",
      "105/105 [==============================] - 0s 832us/step - loss: 0.0260 - val_loss: 0.0194\n",
      "Epoch 47/200\n",
      "105/105 [==============================] - 0s 813us/step - loss: 0.0259 - val_loss: 0.0188\n",
      "Epoch 48/200\n",
      "105/105 [==============================] - 0s 924us/step - loss: 0.0250 - val_loss: 0.0174\n",
      "Epoch 49/200\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.0245 - val_loss: 0.0174\n",
      "Epoch 50/200\n",
      "105/105 [==============================] - 0s 941us/step - loss: 0.0241 - val_loss: 0.0177\n",
      "Epoch 51/200\n",
      "105/105 [==============================] - 0s 858us/step - loss: 0.0236 - val_loss: 0.0189\n",
      "Epoch 52/200\n",
      "105/105 [==============================] - 0s 877us/step - loss: 0.0233 - val_loss: 0.0176\n",
      "Epoch 53/200\n",
      "105/105 [==============================] - 0s 887us/step - loss: 0.0226 - val_loss: 0.0167\n",
      "Epoch 54/200\n",
      "105/105 [==============================] - 0s 891us/step - loss: 0.0221 - val_loss: 0.0161\n",
      "Epoch 55/200\n",
      "105/105 [==============================] - 0s 864us/step - loss: 0.0218 - val_loss: 0.0152\n",
      "Epoch 56/200\n",
      "105/105 [==============================] - 0s 944us/step - loss: 0.0215 - val_loss: 0.0151\n",
      "Epoch 57/200\n",
      "105/105 [==============================] - 0s 898us/step - loss: 0.0209 - val_loss: 0.0144\n",
      "Epoch 58/200\n",
      "105/105 [==============================] - 0s 968us/step - loss: 0.0205 - val_loss: 0.0143\n",
      "Epoch 59/200\n",
      "105/105 [==============================] - 0s 894us/step - loss: 0.0202 - val_loss: 0.0140\n",
      "Epoch 60/200\n",
      "105/105 [==============================] - 0s 836us/step - loss: 0.0198 - val_loss: 0.0138\n",
      "Epoch 61/200\n",
      "105/105 [==============================] - 0s 979us/step - loss: 0.0193 - val_loss: 0.0138\n",
      "Epoch 62/200\n",
      "105/105 [==============================] - 0s 875us/step - loss: 0.0188 - val_loss: 0.0145\n",
      "Epoch 63/200\n",
      "105/105 [==============================] - 0s 856us/step - loss: 0.0187 - val_loss: 0.0132\n",
      "Epoch 64/200\n",
      "105/105 [==============================] - 0s 811us/step - loss: 0.0184 - val_loss: 0.0142\n",
      "Epoch 65/200\n",
      "105/105 [==============================] - 0s 752us/step - loss: 0.0181 - val_loss: 0.0136\n",
      "Epoch 66/200\n",
      "105/105 [==============================] - 0s 831us/step - loss: 0.0176 - val_loss: 0.0123\n",
      "Epoch 67/200\n",
      "105/105 [==============================] - 0s 808us/step - loss: 0.0173 - val_loss: 0.0127\n",
      "Epoch 68/200\n",
      "105/105 [==============================] - 0s 853us/step - loss: 0.0170 - val_loss: 0.0117\n",
      "Epoch 69/200\n",
      "105/105 [==============================] - 0s 790us/step - loss: 0.0169 - val_loss: 0.0122\n",
      "Epoch 70/200\n",
      "105/105 [==============================] - 0s 825us/step - loss: 0.0164 - val_loss: 0.0129\n",
      "Epoch 71/200\n",
      "105/105 [==============================] - 0s 833us/step - loss: 0.0163 - val_loss: 0.0126\n",
      "Epoch 72/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0120\n",
      "Epoch 73/200\n",
      "105/105 [==============================] - 0s 888us/step - loss: 0.0157 - val_loss: 0.0113\n",
      "Epoch 74/200\n",
      "105/105 [==============================] - 0s 852us/step - loss: 0.0155 - val_loss: 0.0110\n",
      "Epoch 75/200\n",
      "105/105 [==============================] - 0s 795us/step - loss: 0.0152 - val_loss: 0.0108\n",
      "Epoch 76/200\n",
      "105/105 [==============================] - 0s 832us/step - loss: 0.0150 - val_loss: 0.0104\n",
      "Epoch 77/200\n",
      "105/105 [==============================] - 0s 819us/step - loss: 0.0148 - val_loss: 0.0103\n",
      "Epoch 78/200\n",
      "105/105 [==============================] - 0s 820us/step - loss: 0.0145 - val_loss: 0.0102\n",
      "Epoch 79/200\n",
      "105/105 [==============================] - 0s 788us/step - loss: 0.0143 - val_loss: 0.0100\n",
      "Epoch 80/200\n",
      "105/105 [==============================] - 0s 813us/step - loss: 0.0141 - val_loss: 0.0102\n",
      "Epoch 81/200\n",
      "105/105 [==============================] - 0s 812us/step - loss: 0.0137 - val_loss: 0.0096\n",
      "Epoch 82/200\n",
      "105/105 [==============================] - 0s 873us/step - loss: 0.0137 - val_loss: 0.0095\n",
      "Epoch 83/200\n",
      "105/105 [==============================] - 0s 803us/step - loss: 0.0135 - val_loss: 0.0094\n",
      "Epoch 84/200\n",
      "105/105 [==============================] - 0s 871us/step - loss: 0.0133 - val_loss: 0.0097\n",
      "Epoch 85/200\n",
      "105/105 [==============================] - 0s 815us/step - loss: 0.0131 - val_loss: 0.0096\n",
      "Epoch 86/200\n",
      "105/105 [==============================] - 0s 841us/step - loss: 0.0130 - val_loss: 0.0094\n",
      "Epoch 87/200\n",
      "105/105 [==============================] - 0s 824us/step - loss: 0.0126 - val_loss: 0.0092\n",
      "Epoch 88/200\n",
      "105/105 [==============================] - 0s 851us/step - loss: 0.0126 - val_loss: 0.0088\n",
      "Epoch 89/200\n",
      "105/105 [==============================] - 0s 909us/step - loss: 0.0125 - val_loss: 0.0087\n",
      "Epoch 90/200\n",
      "105/105 [==============================] - 0s 830us/step - loss: 0.0123 - val_loss: 0.0084\n",
      "Epoch 91/200\n",
      "105/105 [==============================] - 0s 837us/step - loss: 0.0122 - val_loss: 0.0090\n",
      "Epoch 92/200\n",
      "105/105 [==============================] - 0s 884us/step - loss: 0.0121 - val_loss: 0.0084\n",
      "Epoch 93/200\n",
      "105/105 [==============================] - 0s 798us/step - loss: 0.0119 - val_loss: 0.0088\n",
      "Epoch 94/200\n",
      "105/105 [==============================] - 0s 826us/step - loss: 0.0117 - val_loss: 0.0084\n",
      "Epoch 95/200\n",
      "105/105 [==============================] - 0s 986us/step - loss: 0.0116 - val_loss: 0.0082\n",
      "Epoch 96/200\n",
      "105/105 [==============================] - 0s 832us/step - loss: 0.0115 - val_loss: 0.0079\n",
      "Epoch 97/200\n",
      "105/105 [==============================] - 0s 933us/step - loss: 0.0115 - val_loss: 0.0079\n",
      "Epoch 98/200\n",
      "105/105 [==============================] - 0s 927us/step - loss: 0.0113 - val_loss: 0.0088\n",
      "Epoch 99/200\n",
      "105/105 [==============================] - 0s 872us/step - loss: 0.0112 - val_loss: 0.0083\n",
      "Epoch 100/200\n",
      "105/105 [==============================] - 0s 813us/step - loss: 0.0110 - val_loss: 0.0076\n",
      "Epoch 101/200\n",
      "105/105 [==============================] - 0s 856us/step - loss: 0.0109 - val_loss: 0.0076\n",
      "Epoch 102/200\n",
      "105/105 [==============================] - 0s 974us/step - loss: 0.0108 - val_loss: 0.0076\n",
      "Epoch 103/200\n",
      "105/105 [==============================] - 0s 832us/step - loss: 0.0107 - val_loss: 0.0074\n",
      "Epoch 104/200\n",
      "105/105 [==============================] - 0s 860us/step - loss: 0.0106 - val_loss: 0.0075\n",
      "Epoch 105/200\n",
      "105/105 [==============================] - 0s 818us/step - loss: 0.0104 - val_loss: 0.0082\n",
      "Epoch 106/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0103 - val_loss: 0.0072\n",
      "Epoch 107/200\n",
      "105/105 [==============================] - 0s 953us/step - loss: 0.0104 - val_loss: 0.0071\n",
      "Epoch 108/200\n",
      "105/105 [==============================] - 0s 878us/step - loss: 0.0102 - val_loss: 0.0073\n",
      "Epoch 109/200\n",
      "105/105 [==============================] - 0s 831us/step - loss: 0.0102 - val_loss: 0.0069\n",
      "Epoch 110/200\n",
      "105/105 [==============================] - 0s 825us/step - loss: 0.0101 - val_loss: 0.0074\n",
      "Epoch 111/200\n",
      "105/105 [==============================] - 0s 881us/step - loss: 0.0101 - val_loss: 0.0070\n",
      "Epoch 112/200\n",
      "105/105 [==============================] - 0s 791us/step - loss: 0.0099 - val_loss: 0.0070\n",
      "Epoch 113/200\n",
      "105/105 [==============================] - 0s 829us/step - loss: 0.0098 - val_loss: 0.0070\n",
      "Epoch 114/200\n",
      "105/105 [==============================] - 0s 835us/step - loss: 0.0098 - val_loss: 0.0068\n",
      "Epoch 115/200\n",
      "105/105 [==============================] - 0s 840us/step - loss: 0.0097 - val_loss: 0.0069\n",
      "Epoch 116/200\n",
      "105/105 [==============================] - 0s 934us/step - loss: 0.0097 - val_loss: 0.0072\n",
      "Epoch 117/200\n",
      "105/105 [==============================] - 0s 919us/step - loss: 0.0097 - val_loss: 0.0066\n",
      "Epoch 118/200\n",
      "105/105 [==============================] - 0s 943us/step - loss: 0.0095 - val_loss: 0.0072\n",
      "Epoch 119/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0094 - val_loss: 0.0065\n",
      "Epoch 120/200\n",
      "105/105 [==============================] - 0s 953us/step - loss: 0.0095 - val_loss: 0.0068\n",
      "Epoch 121/200\n",
      "105/105 [==============================] - 0s 941us/step - loss: 0.0094 - val_loss: 0.0067\n",
      "Epoch 122/200\n",
      "105/105 [==============================] - 0s 947us/step - loss: 0.0094 - val_loss: 0.0068\n",
      "Epoch 123/200\n",
      "105/105 [==============================] - 0s 893us/step - loss: 0.0093 - val_loss: 0.0065\n",
      "Epoch 124/200\n",
      "105/105 [==============================] - 0s 883us/step - loss: 0.0092 - val_loss: 0.0068\n",
      "Epoch 125/200\n",
      "105/105 [==============================] - 0s 857us/step - loss: 0.0092 - val_loss: 0.0065\n",
      "Epoch 126/200\n",
      "105/105 [==============================] - 0s 897us/step - loss: 0.0091 - val_loss: 0.0080\n",
      "Epoch 127/200\n",
      "105/105 [==============================] - 0s 944us/step - loss: 0.0093 - val_loss: 0.0067\n",
      "Epoch 128/200\n",
      "105/105 [==============================] - 0s 938us/step - loss: 0.0091 - val_loss: 0.0065\n",
      "Epoch 129/200\n",
      "105/105 [==============================] - 0s 930us/step - loss: 0.0091 - val_loss: 0.0067\n",
      "Epoch 130/200\n",
      "105/105 [==============================] - 0s 873us/step - loss: 0.0090 - val_loss: 0.0069\n",
      "Epoch 131/200\n",
      "105/105 [==============================] - 0s 812us/step - loss: 0.0090 - val_loss: 0.0063\n",
      "Epoch 132/200\n",
      "105/105 [==============================] - 0s 824us/step - loss: 0.0089 - val_loss: 0.0062\n",
      "Epoch 133/200\n",
      "105/105 [==============================] - 0s 966us/step - loss: 0.0089 - val_loss: 0.0063\n",
      "Epoch 134/200\n",
      "105/105 [==============================] - 0s 907us/step - loss: 0.0089 - val_loss: 0.0059\n",
      "Epoch 135/200\n",
      "105/105 [==============================] - 0s 815us/step - loss: 0.0089 - val_loss: 0.0064\n",
      "Epoch 136/200\n",
      "105/105 [==============================] - 0s 774us/step - loss: 0.0088 - val_loss: 0.0064\n",
      "Epoch 137/200\n",
      "105/105 [==============================] - 0s 932us/step - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 138/200\n",
      "105/105 [==============================] - 0s 971us/step - loss: 0.0088 - val_loss: 0.0060\n",
      "Epoch 139/200\n",
      "105/105 [==============================] - 0s 785us/step - loss: 0.0087 - val_loss: 0.0058\n",
      "Epoch 140/200\n",
      "105/105 [==============================] - 0s 918us/step - loss: 0.0088 - val_loss: 0.0060\n",
      "Epoch 141/200\n",
      "105/105 [==============================] - 0s 885us/step - loss: 0.0087 - val_loss: 0.0059\n",
      "Epoch 142/200\n",
      "105/105 [==============================] - 0s 919us/step - loss: 0.0087 - val_loss: 0.0061\n",
      "Epoch 143/200\n",
      "105/105 [==============================] - 0s 861us/step - loss: 0.0087 - val_loss: 0.0060\n",
      "Epoch 144/200\n",
      "105/105 [==============================] - 0s 917us/step - loss: 0.0086 - val_loss: 0.0062\n",
      "Epoch 145/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0086 - val_loss: 0.0060\n",
      "Epoch 146/200\n",
      "105/105 [==============================] - 0s 976us/step - loss: 0.0086 - val_loss: 0.0066\n",
      "Epoch 147/200\n",
      "105/105 [==============================] - 0s 856us/step - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 148/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0085 - val_loss: 0.0059\n",
      "Epoch 149/200\n",
      "105/105 [==============================] - 0s 959us/step - loss: 0.0086 - val_loss: 0.0059\n",
      "Epoch 150/200\n",
      "105/105 [==============================] - 0s 922us/step - loss: 0.0085 - val_loss: 0.0057\n",
      "Epoch 151/200\n",
      "105/105 [==============================] - 0s 906us/step - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 152/200\n",
      "105/105 [==============================] - 0s 897us/step - loss: 0.0085 - val_loss: 0.0057\n",
      "Epoch 153/200\n",
      "105/105 [==============================] - 0s 898us/step - loss: 0.0085 - val_loss: 0.0063\n",
      "Epoch 154/200\n",
      "105/105 [==============================] - 0s 804us/step - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 155/200\n",
      "105/105 [==============================] - 0s 871us/step - loss: 0.0084 - val_loss: 0.0058\n",
      "Epoch 156/200\n",
      "105/105 [==============================] - 0s 860us/step - loss: 0.0084 - val_loss: 0.0057\n",
      "Epoch 157/200\n",
      "105/105 [==============================] - 0s 886us/step - loss: 0.0085 - val_loss: 0.0058\n",
      "Epoch 158/200\n",
      "105/105 [==============================] - 0s 881us/step - loss: 0.0084 - val_loss: 0.0057\n",
      "Epoch 159/200\n",
      "105/105 [==============================] - 0s 958us/step - loss: 0.0083 - val_loss: 0.0059\n",
      "Epoch 160/200\n",
      "105/105 [==============================] - 0s 988us/step - loss: 0.0084 - val_loss: 0.0058\n",
      "Epoch 161/200\n",
      "105/105 [==============================] - 0s 846us/step - loss: 0.0083 - val_loss: 0.0057\n",
      "Epoch 162/200\n",
      "105/105 [==============================] - 0s 848us/step - loss: 0.0083 - val_loss: 0.0056\n",
      "Epoch 163/200\n",
      "105/105 [==============================] - 0s 895us/step - loss: 0.0084 - val_loss: 0.0061\n",
      "Epoch 164/200\n",
      "105/105 [==============================] - 0s 890us/step - loss: 0.0083 - val_loss: 0.0060\n",
      "Epoch 165/200\n",
      "105/105 [==============================] - 0s 813us/step - loss: 0.0083 - val_loss: 0.0061\n",
      "Epoch 166/200\n",
      "105/105 [==============================] - 0s 816us/step - loss: 0.0082 - val_loss: 0.0055\n",
      "Epoch 167/200\n",
      "105/105 [==============================] - 0s 768us/step - loss: 0.0083 - val_loss: 0.0055\n",
      "Epoch 168/200\n",
      "105/105 [==============================] - 0s 780us/step - loss: 0.0083 - val_loss: 0.0058\n",
      "Epoch 169/200\n",
      "105/105 [==============================] - 0s 867us/step - loss: 0.0082 - val_loss: 0.0056\n",
      "Epoch 170/200\n",
      "105/105 [==============================] - 0s 953us/step - loss: 0.0083 - val_loss: 0.0058\n",
      "Epoch 171/200\n",
      "105/105 [==============================] - 0s 931us/step - loss: 0.0083 - val_loss: 0.0056\n",
      "Epoch 172/200\n",
      "105/105 [==============================] - 0s 986us/step - loss: 0.0083 - val_loss: 0.0059\n",
      "Epoch 173/200\n",
      "105/105 [==============================] - 0s 899us/step - loss: 0.0082 - val_loss: 0.0059\n",
      "Epoch 174/200\n",
      "105/105 [==============================] - 0s 869us/step - loss: 0.0083 - val_loss: 0.0054\n",
      "Epoch 175/200\n",
      "105/105 [==============================] - 0s 807us/step - loss: 0.0082 - val_loss: 0.0054\n",
      "Epoch 176/200\n",
      "105/105 [==============================] - 0s 856us/step - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 177/200\n",
      "105/105 [==============================] - 0s 798us/step - loss: 0.0083 - val_loss: 0.0063\n",
      "Epoch 178/200\n",
      "105/105 [==============================] - 0s 815us/step - loss: 0.0082 - val_loss: 0.0060\n",
      "Epoch 179/200\n",
      "105/105 [==============================] - 0s 825us/step - loss: 0.0082 - val_loss: 0.0057\n",
      "Epoch 180/200\n",
      "105/105 [==============================] - 0s 821us/step - loss: 0.0083 - val_loss: 0.0055\n",
      "Epoch 181/200\n",
      "105/105 [==============================] - 0s 909us/step - loss: 0.0083 - val_loss: 0.0059\n",
      "Epoch 182/200\n",
      "105/105 [==============================] - 0s 825us/step - loss: 0.0082 - val_loss: 0.0057\n",
      "Epoch 183/200\n",
      "105/105 [==============================] - 0s 878us/step - loss: 0.0081 - val_loss: 0.0058\n",
      "Epoch 184/200\n",
      "105/105 [==============================] - 0s 893us/step - loss: 0.0082 - val_loss: 0.0058\n",
      "Epoch 185/200\n",
      "105/105 [==============================] - 0s 914us/step - loss: 0.0082 - val_loss: 0.0058\n",
      "Epoch 186/200\n",
      "105/105 [==============================] - 0s 769us/step - loss: 0.0082 - val_loss: 0.0057\n",
      "Epoch 187/200\n",
      "105/105 [==============================] - 0s 917us/step - loss: 0.0082 - val_loss: 0.0057\n",
      "Epoch 188/200\n",
      "105/105 [==============================] - 0s 888us/step - loss: 0.0082 - val_loss: 0.0059\n",
      "Epoch 189/200\n",
      "105/105 [==============================] - 0s 816us/step - loss: 0.0082 - val_loss: 0.0060\n",
      "Epoch 190/200\n",
      "105/105 [==============================] - 0s 959us/step - loss: 0.0081 - val_loss: 0.0056\n",
      "Epoch 191/200\n",
      "105/105 [==============================] - 0s 948us/step - loss: 0.0082 - val_loss: 0.0061\n",
      "Epoch 192/200\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0082 - val_loss: 0.0054\n",
      "Epoch 193/200\n",
      "105/105 [==============================] - 0s 921us/step - loss: 0.0081 - val_loss: 0.0061\n",
      "Epoch 194/200\n",
      "105/105 [==============================] - 0s 933us/step - loss: 0.0081 - val_loss: 0.0059\n",
      "Epoch 195/200\n",
      "105/105 [==============================] - 0s 978us/step - loss: 0.0081 - val_loss: 0.0059\n",
      "Epoch 196/200\n",
      "105/105 [==============================] - 0s 943us/step - loss: 0.0081 - val_loss: 0.0057\n",
      "Epoch 197/200\n",
      "105/105 [==============================] - 0s 928us/step - loss: 0.0081 - val_loss: 0.0054\n",
      "Epoch 198/200\n",
      "105/105 [==============================] - 0s 919us/step - loss: 0.0081 - val_loss: 0.0056\n",
      "Epoch 199/200\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.0082 - val_loss: 0.0055\n",
      "Epoch 200/200\n",
      "105/105 [==============================] - 0s 897us/step - loss: 0.0081 - val_loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=4,\n",
    "                    epochs=200,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3_CdUC43g1g"
   },
   "source": [
    "As you can see above, we train our network down to a validation MSE < 0.01. Notice that both the training loss (\"loss\") and validation loss (\"val_loss\") are reported. It's normal for the training loss to be lower than the validation loss, since the network's objective is to predict the training data well. But if the training loss is much lower than our validation loss, it means we are overfitting and may not expect to receive very good results.\n",
    "\n",
    "We can evaluate the training set one last time at the end using `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CjIZX2er3g1h",
    "outputId": "6d567e02-93d4-4316-c6cb-15785b59b125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 209us/step\n",
      "Test loss: 0.005925121758547094\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ucif8nsm3g1l"
   },
   "source": [
    "To get the raw predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "94K-RIWB3g1l",
    "outputId": "b9bf0bc9-c5b7-4ce7-ddd7-5023151b6c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 0.64, actual 0.60\n",
      "predicted 0.73, actual 0.92\n",
      "predicted 0.55, actual 0.48\n",
      "predicted 0.61, actual 0.68\n",
      "predicted 0.61, actual 0.60\n",
      "predicted 0.11, actual 0.12\n",
      "predicted 0.65, actual 0.48\n",
      "predicted 0.79, actual 0.84\n",
      "predicted 0.75, actual 0.76\n",
      "predicted 0.83, actual 0.84\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "for yp, ya in list(zip(y_pred, y_test))[0:10]:\n",
    "    print(\"predicted %0.2f, actual %0.2f\" % (yp, ya))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5mHgPhA3g1n"
   },
   "source": [
    "To visualize the prediction results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "XHAiiY-83g1o",
    "outputId": "5ea17861-f808-49e1-8af9-0f8549f9b429"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5c2ff8e0f0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAGDCAYAAAAyM4nNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndqQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('Actual sepal width (cm)')\n",
    "plt.ylabel('Predicted sepal width (cm)')\n",
    "plt.title('Iris dataset')\n",
    "plt.scatter(y_test, y_pred, c='b')\n",
    "plt.plot([0, 1], [0, 1], '--', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h441UyRw3g1q"
   },
   "source": [
    "We can manually calculate MSE as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fOEUu9LI3g1q",
    "outputId": "5a144220-a2a0-4615-d1e2-ae5a575fb6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 0.0059\n"
     ]
    }
   ],
   "source": [
    "def MSE(y_pred, y_test):\n",
    "    return (1.0/len(y_test)) * np.sum([((y1[0]-y2)**2) for y1, y2 in list(zip(y_pred, y_test))])\n",
    "\n",
    "print(\"MSE is %0.4f\" % MSE(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpmTEYl53g1s"
   },
   "source": [
    "We can also predict the value of a single unknown example or a set of them in th following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h4tN83pm3g1t",
    "outputId": "2e166e6c-f647-4a28-f797-2881c8a7e7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 0.636, actual 0.600\n"
     ]
    }
   ],
   "source": [
    "x_sample = x_test[0].reshape(1, 3)   # shape must be (num_samples, 3), even if num_samples = 1\n",
    "y_prob = model.predict(x_sample)\n",
    "\n",
    "print(\"predicted %0.3f, actual %0.3f\" % (y_prob[0][0], y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35mSMVS93g1v"
   },
   "source": [
    "We've now finished introducing Keras for regression. Note it is a far more powerful way of training neural networks than our own. Keras's strengths will become even more apparent when we introduce classification in the next lesson, as well as introduce convolutional networks and various other optimization tricks it enables for us."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_simple_neural_networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
